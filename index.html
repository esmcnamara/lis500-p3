
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>LIS 500 Teachable Machine Project</title>
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header class="site-header">
    <div class="wrapper">
      <h1>Teachable Machine Project</h1>
      <nav aria-label="Main navigation">
        <ul class="nav">
          <li><a aria-current="page" href="index.html">Home</a></li>
          <li><a href="test.html">Test the Model</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="wrapper">
    <section class="hero">
      <h2>Where Are You Looking!?</h2>
      <p>
        We trained an image classifier to recognize which way you are looking when the camera is on.
      </p>
      <p>
        <a class="btn" href="test.html">Test It! →</a>
      </p>
    </section>

<section class="section" style="text-align:center;">
  <h3>Project Objective</h3>

  <p>Head Direction Recognition</p>
  <p>5 Head Positions</p>
  <p>Positions Included: Left, Right, Up, Down, Center</p>
  <p>Approximately 10-20 images per direction (collected by multiple group members)</p>
  <p><strong>Purpose:</strong><p>
  <p>To demonstrate how machine learning identifies visual patterns based on training examples and how dataset limitations affect accuracy.</p>
</section>

    
  <section class="section">
    <h3>Demo</h3>

    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;max-width:900px;margin-top:12px;">
      <iframe
        src="https://www.youtube.com/embed/aS8vWwXRP-c"
        title="Teachable Machine Demo"
        style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
        allowfullscreen
      ></iframe>
    </div>
  </section>


<section class="section">
  <h3>Project Statement</h3>

  <p>For our group project, we set out to create a small but meaningful machine-learning experiment using Google’s Teachable Machine. Our goal was to build a simple image-based classifier and present it on a website that met the design, accessibility, and content requirements of the assignment. Our classifier shows how machine learning "learns" patterns from examples and focuses on identifying head movements, such as looking left, right, up, down, or straight ahead. We provided a thorough description of the model on our website, along with an embedded demo video recorded by Ethan.</p>

  <p>Recognizing a person's direction of look—left, right, up, down, or straight ahead—is the main focus of our model. We chose this idea because it seemed easy to collect data for and because the Teachable Machine interface is designed around giving examples to the system through video input. At first, training the model felt like just another technical exercise: tilt your head, record a few seconds, click “train.” But as we kept working, we realized this project was really about understanding how machines learn, what they miss, and how the design choices we make end up shaping everything the model “knows.”</p>

  <p>We built the dataset almost entirely on our own from the start. Each of us took turns recording images for the five categories, but because of time, convenience, and lighting constraints, our dataset ended up being far less diverse than we expected. The majority of the training photos were taken in the same location using identical backgrounds, lighting, and camera. Even more importantly, the model mostly saw faces that look like ours — similar skin tones, similar hair colors, similar ages, and similar facial structures. At the time, it didn’t feel like a problem, but while reading <em>Unmasking AI</em> alongside this project, we began to see how these seemingly small choices connect to a bigger pattern in real-world AI systems.</p>

  <p>Throughout <em>Unmasking AI</em>, Buolamwini shows how systems built without diverse data or careful oversight often fail to recognize people who don’t match the majority group in the training set. Despite its modest size, our project clearly reflected that reality. Since Ethan was present in the majority of the training samples, the model performed well when he recorded the demo video. However, we noticed that it is not always accurate when other people tried it. It would sometimes give incorrect classifications; for example, someone who was facing the camera was labeled as “looking left.”</p>

  <p>In the book, Buolamwini introduces the concept of the coded gaze, the idea that algorithms inherit the worldview, the blind spots, and the biases of the people who built them. As we trained our model, we started to understand what this meant on a practical level. Our classifier didn’t learn what “people” look like when they turn their heads; it learned what we look like when we turn our heads. Every limitation in the dataset became a limitation in the system itself. The model nearly always correctly classified Ethan when he recorded the demonstration video for our website, which makes perfect sense because the majority of the training examples featured us in similar lighting and framing. But when we tested the model on someone outside our group or when a different lighting condition was introduced, the model struggled to recognize the direction.</p>

  <p>One of the most important lessons in <em>Unmasking AI</em> was made clear to us by that experience: bias frequently results from convenience, presumptions, and small datasets rather than from malicious intent. We simply chose the easiest method for gathering data; we weren't attempting to create a biased model. Even in high-stakes situations, however, that "easy path" is exactly why it produces unfair results. Buolamwini describes how commercial facial recognition systems couldn’t identify her face unless she wore a white mask, not because her face was difficult to analyze but because those systems were never trained on people who looked like her. In a much smaller way, we saw the same logic in our own model: the technology worked best for the people it was built around and had trouble with anyone outside that narrow group.</p>

  <p>Another major point from <em>Unmasking AI</em> that connected to our work was the emotional impact of misrecognition. Buolamwini writes about how harmful it felt to be “invisible” to systems that were supposed to work for everyone. Our project isn’t dealing with identity verification, policing, hiring, or healthcare, but even in our low-stakes setting, misclassification still revealed something important. It was more than a technical error when the classifier consistently misread a person's head direction. It signaled that the system didn’t “understand” that person in the same way it understood others. This prompted us to consider the ramifications of being misidentified or invisible by technology, particularly for individuals whose identities place them at the intersections of class, ability, gender, race, and other dimensions.</p>

  <p>Transparency was another area where our project aligned with Buolamwini’s ideas. In <em>Unmasking AI</em>, she argues that developers and companies should clearly explain how their systems work, who they fail for, and what their limitations are. On our website, we included a section explaining how the classifier was trained, the types of images we used, the examples we collected, and an honest reflection on where the model struggles. We also uploaded our code to GitHub and linked it so visitors could see exactly what we built. Even adding Ethan’s demo video helped make the project more transparent, because viewers can watch the model in real time and see both its strengths and its weaknesses.</p>

  <p>In conclusion, our Teachable Machine model and website project gave us a hands-on way to experience the challenges and responsibilities of building technology. It helped us see how easily bias appears, how limited data shapes outcomes, and how important inclusive design is. <em>Unmasking AI</em> didn’t just give us background knowledge for the assignment — it completely reshaped how we understood our own project. We discovered that fairness must be consciously created, challenged, and maintained after considering these themes. Even though our model is small, the lessons we learned from it and from Buolamwini's work will stay with us as we continue to research and develop technology.</p>

</section>


    <section class="section">
      <h3>Team</h3>
      <ul>
        <li>Ethan McNamara</li>
        <li>Dominik Torok</li>
        <li></li>
      </ul>
    </section>
  </main>

  <footer class="site-footer">
    <p>Ethan McNamara, Dominik Torok, and</p>
  </footer>
</body>
</html>
