
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>LIS 500 Teachable Machine Project</title>
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header class="site-header">
    <div class="wrapper">
      <h1>Teachable Machine Project</h1>
      <nav aria-label="Main navigation">
        <ul class="nav">
          <li><a aria-current="page" href="index.html">Home</a></li>
          <li><a href="test.html">Test the Model</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="wrapper">
    <section class="hero">
      <h2>Where Are You Looking!?</h2>
      <p>
        We trained an image classifier to recognize which way you are looking when the camera is on.
      </p>
      <p>
        <a class="btn" href="test.html">Test It! →</a>
      </p>
    </section>

<section class="section" style="text-align:center;">
  <h3>Project Objective</h3>

  <p>Head Direction Recognition</p>
  <p>5 Head Positions</p>
  <p>Positions Included: Left, Right, Up, Down,Center</p>
  <p>Approximately 10-20 images per direction (collected by multiple group members)</p>
  <p>Purpose:To demonstrate how machine learning identifies visual patterns based on training examples and how dataset limitations affect accuracy.</p>
</section>

    
  <section class="section">
    <h3>Demo</h3>

    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;max-width:900px;margin-top:12px;">
      <iframe
        src="https://www.youtube.com/embed/aS8vWwXRP-c"
        title="Teachable Machine Demo"
        style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"
        allowfullscreen
      ></iframe>
    </div>
  </section>


    <section class="section">
      <h3>Project statement</h3>
<p>For our group project, we set out to create a small but meaningful machine-learning experiment using Google’s Teachable Machine. Our goal was to build a simple image-based classifier and present it on a website that met the design, accessibility, and content requirements of the assignment. The classifier we created focuses on recognizing head movements, directions like looking left, right, up, down, or straight ahead, and demonstrates how machine learning “learns” patterns from examples. On our site, we included a full explanation of the model, an embedded demo video filmed by Ethan. <p>

<p>Our model focuses on something that sounds straightforward: recognizing the direction someone is looking — left, right, up, down, or straight ahead. We chose this idea because it seemed easy to collect data for and because the Teachable Machine interface is designed around giving examples to the system through video input. At first, training the model felt like just another technical exercise: tilt your head, record a few seconds, click “train.” But as we kept working, we realized this project was really about understanding how machines learn, what they miss, and how the design choices we make end up shaping everything the model “knows.” <p>

<p>From the beginning, we relied almost entirely on ourselves to build the dataset. Each of us took turns recording images for the five categories, but because of time, convenience, and lighting constraints, our dataset ended up being far less diverse than we expected. Most of the training images were taken in the same room, with the same lighting, the same camera, and the same backgrounds. Even more importantly, the model mostly saw faces that look like ours — similar skin tones, similar hair colors, similar ages, and similar facial structures. At the time, it didn’t feel like a problem, but while reading Unmasking AI alongside this project, we began to see how these seemingly small choices connect to a bigger pattern in real-world AI systems. <p>

<p>Throughout Unmasking AI, Buolamwini shows how systems built without diverse data or careful oversight often fail to recognize people who don’t match the majority group in the training set. Our project, although small, mirrored that reality in a clear way. When Ethan recorded the demo video, the model worked smoothly because he appeared in most of the training samples. However when other people tried it, we realized that it is not accurate all the time. It is giving us fake information, for example someone who tried it was facing the camera and the model told us that they are looking to the left. <p>

<p>In the book, Buolamwini introduces the concept of the coded gaze, the idea that algorithms inherit the worldview, the blind spots, and the biases of the people who built them. As we trained our model, we started to understand what this meant on a practical level. Our classifier didn’t learn what “people” look like when they turn their heads; it learned what we look like when we turn our heads. Every limitation in the dataset became a limitation in the system itself. When Ethan recorded the demonstration video for our website, the model classified him correctly almost every time, which makes perfect sense, most of the training examples were of us, in the same lighting, framed in a similar way. But when we tested the model on someone outside our group or when a different lighting condition was introduced, the model struggled to recognize the direction.<p>
That experience helped us understand one of the biggest messages in Unmasking AI: bias doesn’t have to come from bad intentions; it often comes from convenience, assumptions, and limited datasets. We weren’t trying to build a biased model, we just took the easiest path to collecting data. But that “easy path” is exactly what leads to unequal outcomes, even in high-stakes contexts. Buolamwini describes how commercial facial recognition systems couldn’t identify her face unless she wore a white mask, not because her face was difficult to analyze but because those systems were never trained on people who looked like her. In a much smaller way, we saw the same logic in our own model: the technology worked best for the people it was built around and had trouble with anyone outside that narrow group. <p>

<p>Another major point from Unmasking AI that connected to our work was the emotional impact of misrecognition. Buolamwini writes about how harmful it felt to be “invisible” to systems that were supposed to work for everyone. Our project isn’t dealing with identity verification, policing, hiring, or healthcare, but even in our low-stakes setting, misclassification still revealed something important. When the classifier misread someone’s head direction repeatedly, it wasn’t just a technical glitch. It signaled that the system didn’t “understand” that person in the same way it understood others. This made us think about the consequences of being misrecognized or unseen by technology, especially for people whose identities place them at the intersections of race, gender, class, ability, and other dimensions.<p>

<p>Transparency was another area where our project aligned with Buolamwini’s ideas. In Unmasking AI, she argues that developers and companies should clearly explain how their systems work, who they fail for, and what their limitations are. On our website, we included a section explaining how the classifier was trained, the types of images we used, the examples we collected, and an honest reflection on where the model struggles. We also uploaded our code to GitHub and linked it so visitors could see exactly what we built. Even adding Ethan’s demo video helped make the project more transparent, viewers can watch the model in real time and see both its strengths and its weaknesses.<p>

<p>One of the biggest lessons we learned is that building AI responsibly is not about perfection; it’s about intention. Our model will never be perfect, and it was never meant to be. But we now understand why researchers like Buolamwini push for auditing, diverse datasets, regulation, and ethical design. AI systems are used in contexts far more serious than ours — policing, hiring, education, medicine — and if those systems are trained the way we initially trained ours (quickly, conveniently, and narrowly), the consequences can be harmful.<p>
<p>In conclusion, our Teachable Machine model and website project gave us a hands-on way to experience the challenges and responsibilities of building technology. It helped us see how easily bias appears, how limited data shapes outcomes, and how important inclusive design is. Unmasking AI didn’t just give us background knowledge for the assignment, it completely reshaped how we understood our own project. By reflecting on these themes, we learned that fairness isn’t automatic; it has to be intentionally built, questioned, and maintained. And even though our model is small, the lessons we learned from it,  and from Buolamwini’s work, will stay with us as we continue studying and creating technology.<p>
    </section>

    <section class="section">
      <h3>Team</h3>
      <ul>
        <li>Ethan McNamara</li>
        <li>Dominik Torok</li>
        <li></li>
      </ul>
    </section>
  </main>

  <footer class="site-footer">
    <p>Ethan McNamara, Dominik Torok, and</p>
  </footer>
</body>
</html>
